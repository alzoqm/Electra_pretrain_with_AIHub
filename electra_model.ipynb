{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"electra_model.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyMhcbG47BPL7I/apKNY6qOJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","accelerator":"TPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"8iulLf6TU9us","executionInfo":{"status":"ok","timestamp":1659245615546,"user_tz":-540,"elapsed":2795,"user":{"displayName":"nais y","userId":"02059557756502258442"}}},"outputs":[],"source":["import tensorflow as tf\n","import numpy as np\n","import tqdm\n","import os\n","import math\n","import json\n","import matplotlib.pyplot as plt\n","import tensorflow_probability as tfp\n","\n","from random import random, shuffle, choice, randrange\n","from tqdm import tqdm_notebook, tqdm, trange"]},{"cell_type":"code","source":["class MultiHeadAttention(tf.keras.Model):\n","  def __init__(self, d_model, num_heads):\n","    super().__init__()\n","    self.d_model = d_model\n","    self.num_heads = num_heads\n","\n","    assert d_model % num_heads == 0\n","    self.depth = d_model // num_heads\n","\n","    self.q_linear = tf.keras.layers.Dense(d_model)\n","    self.k_linear = tf.keras.layers.Dense(d_model)\n","    self.v_linear = tf.keras.layers.Dense(d_model)\n","\n","    self.linear = tf.keras.layers.Dense(d_model)\n","\n","    self.scale = 1 / (self.depth ** 0.5)\n","\n","  def call(self, inputs, mask=None):\n","    B, L, D = inputs.shape\n","\n","    q = self.q_linear(inputs)\n","    k = self.k_linear(inputs)\n","    v = self.v_linear(inputs)\n","\n","    q = tf.reshape(q, shape=(-1, L, self.num_heads, self.depth))\n","    q = tf.transpose(q, perm=[0, 2, 1, 3])\n","\n","    k = tf.reshape(k, shape=(-1, L, self.num_heads, self.depth))\n","    k = tf.transpose(k, perm=[0, 2, 1, 3])\n","\n","    v = tf.reshape(v, shape=(-1, L, self.num_heads, self.depth))\n","    v = tf.transpose(v, perm=[0, 2, 1, 3])\n","\n","    logits = tf.matmul(q, k, transpose_b=True)\n","\n","    if mask is not None:\n","      logits += (mask*-1e9)\n","    attn_mat = tf.nn.softmax(logits, axis=-1)\n","    output = tf.matmul(attn_mat, v)\n","    output = tf.transpose(output, perm=[0, 2, 1, 3])\n","\n","    output = tf.reshape(output, shape=(-1, L, self.d_model))\n","\n","    output = self.linear(output)\n","\n","    return output, attn_mat"],"metadata":{"id":"1iQCpyg4YEyr","executionInfo":{"status":"ok","timestamp":1659245615547,"user_tz":-540,"elapsed":10,"user":{"displayName":"nais y","userId":"02059557756502258442"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["class MLP(tf.keras.Model):\n","  def __init__(self, dff, d_model, dropout):\n","    super().__init__()\n","    self.linear1 = tf.keras.layers.Dense(dff)\n","    self.linear2 = tf.keras.layers.Dense(d_model)\n","\n","    self.dropout = tf.keras.layers.Dropout(dropout)\n","\n","  def call(self, inputs):\n","    outputs = self.linear1(inputs)\n","    outputs = tf.nn.gelu(outputs)\n","    outputs = self.dropout(outputs)\n","    outputs = self.linear2(outputs)\n","\n","    return outputs"],"metadata":{"id":"7JWtehouYGIh","executionInfo":{"status":"ok","timestamp":1659245615547,"user_tz":-540,"elapsed":9,"user":{"displayName":"nais y","userId":"02059557756502258442"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["class EncoderLayer(tf.keras.Model):\n","  def __init__(self, dff, d_model, num_heads, dropout):\n","    super().__init__()\n","    self.attn_layer = MultiHeadAttention(d_model, num_heads)\n","    self.mlp_layer = MLP(dff, d_model, dropout)\n","    \n","    self.layer_norm1 = tf.keras.layers.LayerNormalization(epsilon=1e-9)\n","    self.layer_norm2 = tf.keras.layers.LayerNormalization(epsilon=1e-9)\n","\n","    self.dropout = tf.keras.layers.Dropout(dropout)\n","\n","  def call(self, inputs, mask):\n","    attn_outputs, attn_mat = self.attn_layer(inputs, mask)\n","    attn_outputs = self.dropout(attn_outputs)\n","    attn_outputs = self.layer_norm1(inputs + attn_outputs)\n","\n","    mlp_outputs = self.mlp_layer(attn_outputs)\n","    mlp_outputs = self.dropout(mlp_outputs)\n","    outputs = self.layer_norm2(attn_outputs + mlp_outputs)\n","\n","    return outputs, attn_mat"],"metadata":{"id":"7vDUmOhMYHH4","executionInfo":{"status":"ok","timestamp":1659245615547,"user_tz":-540,"elapsed":9,"user":{"displayName":"nais y","userId":"02059557756502258442"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["class GeneratorEncoder(tf.keras.Model):\n","  def __init__(self, max_len, seg_type, vocab_size, num_layers, dff, d_model, emb_size, num_heads, dropout):\n","    super().__init__()\n","    # self.word_emb = tf.keras.layers.Embedding(vocab_size, emb_size)\n","    # self.seg_emb = tf.keras.layers.Embedding(seg_type, emb_size)\n","    # self.pos_emb = tf.keras.layers.Embedding(max_len + 1, emb_size) #Discriminator와 동일한 임베딩을 사용 -> 더 좋은 성능을 보여줌\n","\n","    self.encoder_layers = [EncoderLayer(dff, d_model, num_heads, dropout) for i in range(num_layers)]\n","    self.emb_proj = tf.keras.layers.Dense(d_model)\n","  \n","  def create_attn_mask(self, inputs, value):\n","    inputs = tf.cast(tf.math.equal(inputs, value), tf.float32)\n","    return inputs[:, tf.newaxis, tf.newaxis, :]\n","\n","  def call(self, inputs, segments, word_emb, seg_emb, pos_emb):\n","    batch_size = tf.shape(inputs)[0]\n","    pos = np.arange(inputs.shape[1]) + 1\n","    pos = tf.broadcast_to(pos, shape=(batch_size, inputs.shape[1]))\n","    #pos = np.broadcast_to(pos, shape=(batch_size, inputs.shape[1]))\n","    pos_mask = tf.math.equal(inputs, 0)\n","    pos = tf.where(~pos_mask, x=pos, y=0) \n","\n","    outputs = word_emb(inputs) + seg_emb(segments) + pos_emb(pos)\n","    outputs = self.emb_proj(outputs)\n","\n","    attn_mask = self.create_attn_mask(inputs, 0)\n","\n","    attn_mat_list = []\n","    for layer in self.encoder_layers:\n","      outputs, attn_mat = layer(outputs, attn_mask)\n","      attn_mat_list.append(attn_mat)\n","\n","    return outputs, attn_mat_list"],"metadata":{"id":"OKsoqtPgYIuf","executionInfo":{"status":"ok","timestamp":1659245615548,"user_tz":-540,"elapsed":9,"user":{"displayName":"nais y","userId":"02059557756502258442"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["class DiscriminatorEncoder(tf.keras.Model):\n","  def __init__(self, max_len, seg_type, vocab_size, num_layers, dff, d_model, emb_size, num_heads, dropout):\n","    super().__init__()\n","    self.word_emb = tf.keras.layers.Embedding(vocab_size, emb_size)\n","    self.seg_emb = tf.keras.layers.Embedding(seg_type, emb_size)\n","    self.pos_emb = tf.keras.layers.Embedding(max_len + 1, emb_size)\n","\n","    self.encoder_layers = [EncoderLayer(dff, d_model, num_heads, dropout) for i in range(num_layers)]\n","    self.emb_proj = tf.keras.layers.Dense(d_model)\n","  \n","  def create_attn_mask(self, inputs, value):\n","    inputs = tf.cast(tf.math.equal(inputs, value), tf.float32)\n","    return inputs[:, tf.newaxis, tf.newaxis, :]\n","\n","  def call(self, inputs, segments):\n","    batch_size = tf.shape(inputs)[0]\n","    pos = np.arange(inputs.shape[1]) + 1\n","    pos = tf.broadcast_to(pos, shape=(batch_size, inputs.shape[1]))\n","    #pos = np.broadcast_to(pos, shape=(batch_size, inputs.shape[1]))\n","    pos_mask = tf.math.equal(inputs, 0)\n","    pos = tf.where(~pos_mask, x=pos, y=0) \n","\n","    outputs = self.word_emb(inputs) + self.seg_emb(segments) + self.pos_emb(pos)\n","    outputs = self.emb_proj(outputs)\n","\n","    attn_mask = self.create_attn_mask(inputs, 0)\n","\n","    attn_mat_list = []\n","    for layer in self.encoder_layers:\n","      outputs, attn_mat = layer(outputs, attn_mask)\n","      attn_mat_list.append(attn_mat)\n","\n","    return outputs, attn_mat_list"],"metadata":{"id":"s__L-AgQJzbG","executionInfo":{"status":"ok","timestamp":1659245615548,"user_tz":-540,"elapsed":9,"user":{"displayName":"nais y","userId":"02059557756502258442"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["class Discriminator(tf.keras.Model):\n","  def __init__(self, max_len, seg_type, vocab_size, num_layers, dff, d_model, emb_size, num_heads, dropout):\n","    super().__init__()\n","    self.encoder = DiscriminatorEncoder(max_len, seg_type, vocab_size, num_layers, dff, d_model, emb_size, num_heads, dropout)\n","    self.linear = tf.keras.layers.Dense(d_model)\n","\n","  def call(self, inputs, segments):\n","    outputs, attn_mat_list = self.encoder(inputs, segments)\n","    outputs_cls = outputs[:, 0]\n","    outputs = self.linear(outputs)\n","    outputs = tf.nn.gelu(outputs)\n","    \n","    return outputs, outputs_cls, attn_mat_list\n","\n","  def save(self, path):\n","    self.save_weights(path, overwrite=True)\n","  \n","\n","  def load(self, path):\n","    self.load_weights(path)"],"metadata":{"id":"AfgJhF-lKLhD","executionInfo":{"status":"ok","timestamp":1659245615548,"user_tz":-540,"elapsed":9,"user":{"displayName":"nais y","userId":"02059557756502258442"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["class Generator(tf.keras.Model):\n","  def __init__(self, max_len, seg_type, vocab_size, num_layers, dff, d_model, emb_size, num_heads, dropout):\n","    super().__init__()\n","    self.encoder = GeneratorEncoder(max_len, seg_type, vocab_size, num_layers, dff, d_model, emb_size, num_heads, dropout)\n","    self.linear = tf.keras.layers.Dense(d_model)\n","\n","  def call(self, inputs, segments,  word_emb, seg_emb, pos_emb):\n","    outputs, attn_mat_list = self.encoder(inputs, segments,  word_emb, seg_emb, pos_emb)\n","    outputs_cls = outputs[:, 0]\n","    outputs_cls = self.linear(outputs_cls)\n","    outputs_cls = tf.nn.gelu(outputs_cls)\n","    \n","    return outputs, outputs_cls, attn_mat_list\n","\n","  def save(self, path):\n","    self.save_weights(path, overwrite=True)\n","  \n","\n","  def load(self, path):\n","    self.load_weights(path)"],"metadata":{"id":"B4rtx_5tKjIq","executionInfo":{"status":"ok","timestamp":1659245615548,"user_tz":-540,"elapsed":8,"user":{"displayName":"nais y","userId":"02059557756502258442"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["class ElectraPretrain(tf.keras.Model):\n","  def __init__(self, config, **kwargs):\n","    super().__init__(**kwargs)\n","    self.generator = Generator(config['max_len'], config['seg_type'], config['vocab_size'], config['gen_num_layers'], config['gen_dff'], config['gen_d_model'], config['gen_emb_size'],\n","                               config['gen_num_heads'], config['gen_dropout'])\n","    \n","    self.discriminator = Discriminator(config['max_len'], config['seg_type'], config['vocab_size'], config['dis_num_layers'], config['dis_dff'], config['dis_d_model'], config['dis_emb_size'],\n","                                       config['dis_num_heads'], config['dis_dropout'])\n","    \n","    self.word_emb = self.discriminator.encoder.word_emb\n","    self.seg_emb = self.discriminator.encoder.seg_emb\n","    self.pos_emb = self.discriminator.encoder.pos_emb\n","    \n","    self.gen_linear_cls = tf.keras.layers.Dense(config['num_classes'], use_bias=False)\n","    self.gen_linear_lm = tf.keras.layers.Dense(config['vocab_size'], use_bias=False)\n","\n","    self.dis_linear = tf.keras.layers.Dense(2, activation='sigmoid') #시그모이드 활성화 함수를 활용해 마지막 분류\n","    self.dis_lambda = config['dis_lambda']\n","\n","  def gen_loss_function(self, labels_lm, labels_cls, logits_lm, logits_cls, inputs):\n","    mask = tf.math.not_equal(labels_lm, -1)  # masking한 곳은 True\n","    mask_float = tf.cast(mask, dtype=tf.float32)\n","    labels_lm_custom = tf.where(mask, x=labels_lm, y=inputs)  #mask가 True인 곳을 lm으로, 아닌 곳은 inputs -> -1일 모두 제거\n","\n","    loss_cls = tf.losses.sparse_categorical_crossentropy(labels_cls, logits_cls)\n","    loss_cls = tf.reshape(loss_cls, shape=(-1, 1))\n","    loss_lm = tf.losses.sparse_categorical_crossentropy(labels_lm_custom, logits_lm)\n","\n","    loss_lm = tf.multiply(loss_lm, mask_float)  #masking한 위치의 mask_float 값은 1, 아닌 곳은 0이다.\n","    loss = loss_lm + loss_cls\n","    return loss, mask, labels_lm_custom  #mask와 labels_lm_custom은 discriminator의 input을 만드는 sampling 과정에 사용하기 위해\n","\n","  def call(self, inputs):\n","    input, segments, labels_cls, labels_lm = inputs\n","    gen_outputs, gen_outputs_cls, gen_attn_mat_list = self.generator(input, segments, self.word_emb, self.seg_emb, self.pos_emb)\n","    gen_outputs_cls = self.gen_linear_cls(gen_outputs_cls)  #앞 뒤의 문장이 동일한 연결이 되는 문장인지 판별\n","    gen_outputs = self.gen_linear_lm(gen_outputs)\n","\n","    gen_loss, mask, labels_lm_custom = self.gen_loss_function(labels_lm, labels_cls, gen_outputs, gen_outputs_cls, input)\n","    ##sampling\n","    #labels_lm_custom -> 모든 문장이 다 들어가있음\n","    gen_outputs_softmax = tf.nn.softmax(gen_outputs, axis=2)\n","    sampling = tf.constant([], dtype=tf.int32)\n","    for index in range(tf.shape(gen_outputs_softmax)[0]):\n","      \"\"\"\n","      softmax 확률에 따른 sampling을 진행, 2차원 텐서로만 입력을 받기 때문에,\n","       for문을 통해 batch size만큼 반복(더 효과적인 방법 찾아봐야 할 듯)\n","      \"\"\"\n","      one_batch_sampling = tf.random.categorical(gen_outputs_softmax[index], 1, dtype=tf.int32)\n","      one_batch_sampling = tf.expand_dims(one_batch_sampling, axis=0)\n","      if index == 0: #-> predict할 때 문제 발생(해결해야함)\n","        sampling = one_batch_sampling\n","      else:\n","        sampling = tf.concat([sampling, one_batch_sampling], axis=0)\n","    sampling = tf.squeeze(sampling, axis=2)\n","\n","    sampling = tf.where(mask, x=sampling, y = labels_lm_custom) # mask로 대체된 token을 sampling 값으로 변경\n","    \"\"\"\n","    sampling 과정에서 정답과 동일한 token으로 sampling된 경우와 mask가 아닌 값은 True로(Real),\n","    다른 값으로 sampling된 경우는 False(Fake)\n","    \"\"\"\n","    same_token_mask = sampling == labels_lm_custom #same_token_mask는 discriminator의 label값이다.\n","    same_token_mask = tf.cast(same_token_mask, dtype=tf.float32)\n","\n","    dis_outputs, dis_outputs_cls, dis_attn_mat_list = self.discriminator(sampling, segments)\n","    dis_outputs = self.dis_linear(dis_outputs)\n","\n","    dis_loss = tf.losses.sparse_categorical_crossentropy(same_token_mask, dis_outputs)\n","    total_loss = gen_loss + dis_loss*self.dis_lambda\n","\n","    return total_loss, gen_loss, sampling, dis_loss"],"metadata":{"id":"JGitahOqKtbn","executionInfo":{"status":"ok","timestamp":1659245615548,"user_tz":-540,"elapsed":8,"user":{"displayName":"nais y","userId":"02059557756502258442"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["if __name__ == '__main__':\n","  #model hyper parameter는 electra small++과 동일하게 설정\n","  config = {}\n","  config['num_classes'] = 2\n","  config['max_len'] = 512\n","  config['seg_type'] = 2\n","  config['vocab_size'] = 12007\n","  config['gen_num_layers'] = 12\n","  config['gen_dff'] = 1024\n","  config['gen_d_model'] = 256\n","  config['gen_emb_size'] = 128\n","  config['gen_num_heads'] = 4\n","  config['gen_dropout'] = 0.1\n","  config['dis_num_layers'] = 12\n","  config['dis_dff'] = 1024\n","  config['dis_d_model'] = 256\n","  config['dis_emb_size'] = 128\n","  config['dis_num_heads'] = 4\n","  config['dis_dropout'] = 0.1\n","  config['dis_lambda'] = 50\n","\n","  electra_pretrain = ElectraPretrain(config)\n","\n","  x = tf.range(512*10)\n","  x = tf.reshape(x, shape=(-1, 512))\n","  test_batch_size = x.shape[0]\n","  seg = tf.ones_like(x)\n","  label_cls = tf.ones(shape=(test_batch_size, ))\n","  total_loss, gen_loss, sampling, dis_loss = electra_pretrain([x, seg, label_cls, x])\n","  print(gen_loss.shape)\n","  print(dis_loss.shape)"],"metadata":{"executionInfo":{"status":"ok","timestamp":1659245619505,"user_tz":-540,"elapsed":3964,"user":{"displayName":"nais y","userId":"02059557756502258442"}},"id":"enDnCVI6X7FD","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5c19108c-126d-404d-8606-8f2d1f594807"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["(10, 512)\n","(10, 512)\n"]}]}]}