{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "#import transformers as tran\n",
        "import tensorflow as tf\n",
        "import tqdm\n",
        "import random\n",
        "import transformers\n",
        "\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "7tVkpc-kzMLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 데이터 구성\n",
        "랜덤 연결 문장과 다음 연결 문장 모두 10000개의 폴더에 담겨있음.<br>\n",
        "모든 설명에서 next prediction시에 참인 문장들은 'next'로 표현되며, 거짓인 문장들은 'random'으로 표현합니다."
      ],
      "metadata": {
        "id": "J7lV9v58ynfU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_json_list(n_path, r_path):\n",
        "  '''\n",
        "  n_path_list: next 문장들이 저장된 json folder 주소\n",
        "  r_path_list: random 문장들이 저장된 json folder 주소\n",
        "  '''\n",
        "  n_path_list = os.listdir(n_path)\n",
        "  r_path_list = os.listdir(r_path)\n",
        "  return n_path_list, r_path_list"
      ],
      "metadata": {
        "id": "1JxipAq5vUgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def check_file_sen_len(file_path, path_list, dont_want_all_print=False):\n",
        "  '''\n",
        "  폴더에 저장된 파일의 문장의 갯수를 파일별로 출력합니다.\n",
        "  모두 출력후, 모든 파일의 문장 갯수의 합과 평균을 출력합니다.\n",
        "  dont_want_all_print: True일 경우, 모든 파일의 합과 평균만 출력합니다. 기본값은 False.\n",
        "  '''\n",
        "  sum_sen_cnt = 0\n",
        "  num_file = len(path_list)\n",
        "  for index in path_list:\n",
        "    with open(file_path + '/' + index, 'r') as f:\n",
        "      json_file = json.load(f)\n",
        "    if dont_want_all_print == False:\n",
        "      print(f\"{index} 문장 갯수: {len(json_file['sentence'])}\")\n",
        "    sum_sen_cnt += len(json_file['sentence'])\n",
        "  \n",
        "  print(f\"총 문장 갯수: {sum_sen_cnt}\")\n",
        "  print(f\"평균 문장 갯수: {sum_sen_cnt/num_file}\")"
      ],
      "metadata": {
        "id": "wOFVYQz8Ex8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_segment(sen_list, max_len, padding_value, n_or_r=True):\n",
        "  '''\n",
        "  segment와 label_cls를 추가합니다. n_or_r 값을 통해 cls값을 추가할 수 있습니다.\n",
        "  sen_list 구성: (([sen1, label1], [sen2, label2]), ......)\n",
        "  n_or_r: next sen list일 경우 True. random일 경우 False\n",
        "  '''\n",
        "  #######sep 확인 후 코드 수정이 필요########\n",
        "  new_sen_list = []\n",
        "  segments = []\n",
        "  labels_cls = []\n",
        "  labels_lm = []\n",
        "  for data in sen_list:\n",
        "    data1, data2 = data\n",
        "    sen1, label1 = data1\n",
        "    sen2, label2 = data2\n",
        "    if len(sen1) != len(label1): #첫 코드 실수로 길이가 안맞음... sentence에서 [cls] 토큰 빼는 것을 까먹음\n",
        "      sen1 = sen1[1:]\n",
        "    if len(sen2) != len(label2):\n",
        "      sen2 = sen2[1:]\n",
        "    segment1 = np.zeros_like(sen1).tolist()\n",
        "    segment2 = np.ones_like(sen2).tolist()\n",
        "    sen = [[2] + sen1 + sen2]\n",
        "    label_lm = [[-1] + label1 + label2]\n",
        "    segment = [[0] + segment1 + segment2]\n",
        "    # print(\"sen: \" + str(len(sen[0])))\n",
        "    # print(\"label: \" + str(len(label_lm[0])))\n",
        "    sen = tf.keras.preprocessing.sequence.pad_sequences(sen, maxlen=max_len, padding='post', value=padding_value)\n",
        "    label_lm = tf.keras.preprocessing.sequence.pad_sequences(label_lm, maxlen=max_len, padding='post', value=-1) #ignore index: -1\n",
        "    segment = tf.keras.preprocessing.sequence.pad_sequences(segment, maxlen=max_len, padding='post', value=padding_value)\n",
        "\n",
        "    if n_or_r == True: #next일 경우\n",
        "      new_sen_list.append(sen[0])\n",
        "      segments.append(segment[0])\n",
        "      labels_cls.append(1)\n",
        "      labels_lm.append(label_lm[0])\n",
        "    else: #random일 경우\n",
        "      new_sen_list.append(sen[0])\n",
        "      segments.append(segment[0])\n",
        "      labels_cls.append(0)\n",
        "      labels_lm.append(label_lm[0])\n",
        "  return np.array(new_sen_list), np.array(segments), np.array(labels_cls), np.array(labels_lm)"
      ],
      "metadata": {
        "id": "E2d_WuHwWrzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_dataset(n_file_index, r_file_index, n_path_list, r_path_list, n_path, r_path, max_len, padding_value):\n",
        "  '''\n",
        "  n_file_index: n_path_list 중 하나의 index를 나타냅니다.\n",
        "  r_file_index: r_path_list 중 하나의 index를 나타냅니다.\n",
        "  n_path_list: next json 파일의 경로 list파일입니다.\n",
        "  r_path_list: random json 파일의 경로 list파일입니다.\n",
        "  n_path: next 파일들이 저장되어 있는 folder의 경로입니다.\n",
        "  n_path: random 파일들이 저장되어 있는 folder의 경로입니다.\n",
        "  '''\n",
        "  with open(n_path + '/' + n_path_list[n_file_index], 'r') as n_file:\n",
        "    n_json_file = json.load(n_file)\n",
        "  with open(r_path + '/' + r_path_list[n_file_index], 'r') as r_file:\n",
        "    r_json_file = json.load(r_file)\n",
        "\n",
        "  np.random.shuffle(r_json_file['sentence']) #step1 전처리에서 분야별로 파일에 축적이 되기 때문에, 다른 분야와도 섞일수 있도록 합니다.\n",
        "\n",
        "  n_sen_list = n_json_file['sentence'] #-> 구성 ([sen, label], [sen, label])\n",
        "  r_sen_list = [] #-> 구성 [sen, label] 위와 같이 변경 필요.\n",
        "  for index, sen in enumerate(r_json_file['sentence']): \n",
        "    if index % 2 == 0:\n",
        "      temp_sen = sen\n",
        "    else: #2문장씩 묶기\n",
        "      sen = (temp_sen, sen) #-> 구성 ([sen, label], [sen, label])으로 변경\n",
        "      r_sen_list.append(sen)\n",
        "  n_sen_list, n_segments, n_labels_cls, n_labels_lm = make_segment(n_sen_list, max_len, padding_value, True) #-> segment와 label_cls 추가. 구성 (sen, segment, label_cls, label_lm)으로 변경\n",
        "  r_sen_list, r_segments, r_labels_cls, r_labels_lm = make_segment(r_sen_list, max_len, padding_value, False)\n",
        "\n",
        "  sen_list = np.concatenate([n_sen_list, r_sen_list], axis=0)\n",
        "  segments = np.concatenate([n_segments, r_segments], axis=0)\n",
        "  labels_cls = np.concatenate([n_labels_cls, r_labels_cls], axis=0)\n",
        "  labels_lm = np.concatenate([n_labels_lm, r_labels_lm], axis=0)\n",
        "  return sen_list, segments, labels_cls, labels_lm"
      ],
      "metadata": {
        "id": "zQDfdatZVDnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#sen_list, segments, labels_cls, labels_lm = make_dataset(0, 0, n_path_list, r_path_list, n_path, r_path, 512, 0)"
      ],
      "metadata": {
        "id": "C_oF-pUuhrFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UfKVJoWj12_2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}